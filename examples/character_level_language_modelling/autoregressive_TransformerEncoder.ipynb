{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556949\n",
      "ï»¿The Project Gutenberg EBook of Chess Strategy, by Edward Lasker\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org/license\n",
      "\n",
      "\n",
      "Title: Chess Strategy\n",
      "\n",
      "Author: Edward Lasker\n",
      "\n",
      "Translator: J. Du Mont\n",
      "\n",
      "Release Date: November 11, 2012 [EBook #5614]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK CHESS STRATEGY ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by John Mamoun <mamounjo@umdnj.edu>, Charles\n",
      "Franks, and the Online Distributed Proofreaders website.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INFORMATION ABOUT THIS E-TEXT EDITION\n",
      "\n",
      "\n",
      "\n",
      "The following is an e-text of \"Chess Strategy,\" second edition, (1915)\n",
      "by Edward Lasker, translated by J. Du Mont.\n",
      "\n",
      "This e-text contains the 167 chess and checkers board game\n",
      "diagrams appearing in the original book, all in the form of\n",
      "ASCII line drawings. The following is a key to the diagrams:\n",
      "\n",
      "For chess pieces\n"
     ]
    }
   ],
   "source": [
    "# chess\n",
    "\n",
    "with open(\"data/chess_book.txt\", encoding = \"utf-8\") as file:\n",
    "    text = file.read()\n",
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556949"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(text)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2i = {c:i for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "char2i[\"<PAD>\"] = 0\n",
    "char2i[\"<START>\"] = 1\n",
    "char2i[\"<END>\"] = 2\n",
    "i2char = {i:c for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "i2char[0] = \"<PAD>\"\n",
    "i2char[1] = \"<START>\"\n",
    "i2char[2] = \"<END>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556949\n",
      "556932\n",
      "['\\ufeffThe Project Gute', 'The Project Guten', 'he Project Gutenb', 'e Project Gutenbe', ' Project Gutenber']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdfdcfbe24346cfb502f89147904732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=556932.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([556932, 17])\n"
     ]
    }
   ],
   "source": [
    "length = 17\n",
    "lines = []\n",
    "for i in range(len(text))[:-length]:\n",
    "    lines.append(text[i:length + i])\n",
    "print(len(text))\n",
    "print(len(lines))\n",
    "print(lines[:5])\n",
    "encoded = torch.tensor([[char2i[c] for c in l] for l in tqdm(lines)]).to(device).long()\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive Transformer Encoder\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Max sequence length: 16\n",
      "Embedding dimension: 32\n",
      "Feedforward dimension: 128\n",
      "Layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 44,799\n",
      "\n",
      "Epoch | Train                 | Minutes\n",
      "      | Loss     | Error Rate |\n",
      "---------------------------------------\n",
      "    1 |   1.3970 |     39.021 |     0.8\n",
      "    2 |   1.1669 |     34.161 |     1.6\n",
      "    3 |   1.1244 |     33.149 |     2.4\n",
      "    4 |   1.1028 |     32.608 |     3.2\n",
      "    5 |   1.0887 |     32.270 |     4.0\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.TransformerEncoder(char2i, i2char)\n",
    "net.to(device)\n",
    "performance = net.fit(encoded, save_path = \"checkpoints/autoregressive_transformer.pt\", progress_bar = 0)\n",
    "net.save_architecture(\"architectures/autoregressive_transformer.architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_error_rate</th>\n",
       "      <th>minutes</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>model</th>\n",
       "      <th>max_sequence_length</th>\n",
       "      <th>embedding_dimension</th>\n",
       "      <th>feedforward_dimension</th>\n",
       "      <th>layers</th>\n",
       "      <th>attention_heads</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.396962</td>\n",
       "      <td>39.021460</td>\n",
       "      <td>0.780697</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive Transformer Encoder</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.166885</td>\n",
       "      <td>34.161172</td>\n",
       "      <td>1.583761</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive Transformer Encoder</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.124363</td>\n",
       "      <td>33.148661</td>\n",
       "      <td>2.385385</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive Transformer Encoder</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.102792</td>\n",
       "      <td>32.608492</td>\n",
       "      <td>3.189483</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive Transformer Encoder</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.088718</td>\n",
       "      <td>32.269503</td>\n",
       "      <td>4.013829</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive Transformer Encoder</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  train_error_rate   minutes  learning_rate  weight_decay  \\\n",
       "0      1    1.396962         39.021460  0.780697          0.001             0   \n",
       "1      2    1.166885         34.161172  1.583761          0.001             0   \n",
       "2      3    1.124363         33.148661  2.385385          0.001             0   \n",
       "3      4    1.102792         32.608492  3.189483          0.001             0   \n",
       "4      5    1.088718         32.269503  4.013829          0.001             0   \n",
       "\n",
       "                                model  max_sequence_length  \\\n",
       "0  Autoregressive Transformer Encoder                   16   \n",
       "1  Autoregressive Transformer Encoder                   16   \n",
       "2  Autoregressive Transformer Encoder                   16   \n",
       "3  Autoregressive Transformer Encoder                   16   \n",
       "4  Autoregressive Transformer Encoder                   16   \n",
       "\n",
       "   embedding_dimension  feedforward_dimension  layers  attention_heads  \\\n",
       "0                   32                    128       2                2   \n",
       "1                   32                    128       2                2   \n",
       "2                   32                    128       2                2   \n",
       "3                   32                    128       2                2   \n",
       "4                   32                    128       2                2   \n",
       "\n",
       "  activation  dropout  parameters  \n",
       "0       relu      0.0       44799  \n",
       "1       relu      0.0       44799  \n",
       "2       relu      0.0       44799  \n",
       "3       relu      0.0       44799  \n",
       "4       relu      0.0       44799  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# includes all the information about the epoch and the model, useful for reproducibility\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gute',\n",
       " 'The Project Guten',\n",
       " 'he Project Gutenb',\n",
       " 'e Project Gutenbe',\n",
       " ' Project Gutenber']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the input for testing\n",
    "\n",
    "net.tensor2text(encoded[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive Transformer Encoder\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Max sequence length: 16\n",
      "Embedding dimension: 32\n",
      "Feedforward dimension: 128\n",
      "Layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 44,799\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../autoregressive.py:356: RuntimeWarning: Max sequence length exceded, only using the last 16 tokens of the input. You can disable this warning with the warn_last_tokens parameter of the forward method.\n",
      "  warnings.warn(f\"Max sequence length exceded, only using the last {self.architecture['max_sequence_length']} tokens of the input. You can disable this warning with the warn_last_tokens parameter of the forward method.\", category = RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gutenberg-tm order to th',\n",
       " 'The Project Gutenberg-tm order to the',\n",
       " 'he Project Gutenberg-tm order to the ',\n",
       " 'e Project Gutenberg-tm order to play ',\n",
       " ' Project Gutenberg-tm order to play P']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.load_architecture(\"architectures/autoregressive_transformer.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/autoregressive_transformer.pt\"))\n",
    "net.to(device)\n",
    "\n",
    "idx, log_probabilities = net.predict(encoded[:5], main_progress_bar = False, progress_bar = 0)\n",
    "\n",
    "net.tensor2text(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive Transformer Encoder\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Max sequence length: 16\n",
      "Embedding dimension: 32\n",
      "Feedforward dimension: 128\n",
      "Layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 44,799\n",
      "\n",
      "tensor([ -9.3253,  -9.6260,  -9.7838, -11.7067, -12.6741], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gutenberg-tm order to th',\n",
       " 'The Project Gutenberg-tm order to the',\n",
       " 'he Project Gutenberg-tm order to the ',\n",
       " 'e Project Gutenberg-tm order to the K',\n",
       " ' Project Gutenberg-tm order to the Ki']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.load_architecture(\"architectures/autoregressive_transformer.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/autoregressive_transformer.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.greedy_search(encoded[:5], progress_bar = False)\n",
    "\n",
    "print(log_probabilities)\n",
    "net.tensor2text(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive Transformer Encoder\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Max sequence length: 16\n",
      "Embedding dimension: 32\n",
      "Feedforward dimension: 128\n",
      "Layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 44,799\n",
      "\n",
      "tensor([-20.0712, -21.3544, -22.5175, -30.8438, -13.7993], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gutenberg-tm a shilst.\\n\\n',\n",
       " 'The Project Gutenberg-tm c5. K-Kt2   ',\n",
       " 'he Project Gutenberg-tmment.\\n\\nA is ar',\n",
       " 'e Project Gutenberg-tm ouf very wwins',\n",
       " ' Project Gutenberg-tm the weakness th']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.load_architecture(\"architectures/autoregressive_transformer.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/autoregressive_transformer.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.sample(encoded[:5], progress_bar = False)\n",
    "\n",
    "print(log_probabilities)\n",
    "net.tensor2text(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive Transformer Encoder\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Max sequence length: 16\n",
      "Embedding dimension: 32\n",
      "Feedforward dimension: 128\n",
      "Layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 44,799\n",
      "\n",
      "tensor([[ -9.3253, -10.2871, -10.6251, -10.6906, -10.7245],\n",
      "        [ -9.6260, -10.7373, -10.9493, -11.0146, -11.1352],\n",
      "        [ -9.7838, -10.7810, -11.0273, -11.1860, -11.8575],\n",
      "        [-11.0765, -11.2367, -11.7067, -12.1978, -12.2495],\n",
      "        [-12.1441, -12.2229, -12.6741, -12.6960, -13.1410]], device='cuda:0')\n",
      "[['\\ufeffThe Project Gutenberg-tm order to th',\n",
      "  '\\ufeffThe Project Gutenberg-tm the pawns o',\n",
      "  '\\ufeffThe Project Gutenberg-tm the pawns i',\n",
      "  '\\ufeffThe Project Gutenberg-tm the pawns t',\n",
      "  '\\ufeffThe Project Gutenberg-tm order to pl'],\n",
      " ['The Project Gutenberg-tm order to the',\n",
      "  'The Project Gutenberg-tm order to pla',\n",
      "  'The Project Gutenberg-tm the pawns th',\n",
      "  'The Project Gutenberg-tm the pawns in',\n",
      "  'The Project Gutenberg-tm the pawns of'],\n",
      " ['he Project Gutenberg-tm order to the ',\n",
      "  'he Project Gutenberg-tm order to play',\n",
      "  'he Project Gutenberg-tm the pawns the',\n",
      "  'he Project Gutenberg-tm the pawns of ',\n",
      "  'he Project Gutenberg-tm the pawns in '],\n",
      " ['e Project Gutenberg-tm order to play ',\n",
      "  'e Project Gutenberg-tm the pawns the ',\n",
      "  'e Project Gutenberg-tm order to the K',\n",
      "  'e Project Gutenberg-tm order to the p',\n",
      "  'e Project Gutenberg-tm the pawns of t'],\n",
      " [' Project Gutenberg-tm order to play P',\n",
      "  ' Project Gutenberg-tm the pawns of th',\n",
      "  ' Project Gutenberg-tm order to the Ki',\n",
      "  ' Project Gutenberg-tm order to the pa',\n",
      "  ' Project Gutenberg-tm order to play K']]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.load_architecture(\"architectures/autoregressive_transformer.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/autoregressive_transformer.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.beam_search(encoded[:5], progress_bar = 0)\n",
    "\n",
    "print(log_probabilities)\n",
    "pprint([net.tensor2text(t) for t in indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
