{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556949\n",
      "ï»¿The Project Gutenberg EBook of Chess Strategy, by Edward Lasker\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org/license\n",
      "\n",
      "\n",
      "Title: Chess Strategy\n",
      "\n",
      "Author: Edward Lasker\n",
      "\n",
      "Translator: J. Du Mont\n",
      "\n",
      "Release Date: November 11, 2012 [EBook #5614]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK CHESS STRATEGY ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by John Mamoun <mamounjo@umdnj.edu>, Charles\n",
      "Franks, and the Online Distributed Proofreaders website.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INFORMATION ABOUT THIS E-TEXT EDITION\n",
      "\n",
      "\n",
      "\n",
      "The following is an e-text of \"Chess Strategy,\" second edition, (1915)\n",
      "by Edward Lasker, translated by J. Du Mont.\n",
      "\n",
      "This e-text contains the 167 chess and checkers board game\n",
      "diagrams appearing in the original book, all in the form of\n",
      "ASCII line drawings. The following is a key to the diagrams:\n",
      "\n",
      "For chess pieces\n"
     ]
    }
   ],
   "source": [
    "# chess\n",
    "\n",
    "with open(\"data/chess_book.txt\", encoding = \"utf-8\") as file:\n",
    "    text = file.read()\n",
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556949"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(text)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "char2i = {c:i for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "char2i[\"<PAD>\"] = 0\n",
    "char2i[\"<START>\"] = 1\n",
    "char2i[\"<END>\"] = 2\n",
    "print(len(char2i))\n",
    "i2char = {i:c for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "i2char[0] = \"<PAD>\"\n",
    "i2char[1] = \"<START>\"\n",
    "i2char[2] = \"<END>\"\n",
    "print(len(i2char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556949\n",
      "556919\n",
      "['\\ufeffThe Project Gutenberg EBook o', 'The Project Gutenberg EBook of', 'he Project Gutenberg EBook of ', 'e Project Gutenberg EBook of C', ' Project Gutenberg EBook of Ch']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085f21aa6f9449b892488af2e5be1b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=556919.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([556919, 30])\n",
      "torch.Size([556919, 15])\n",
      "torch.Size([556919, 16])\n"
     ]
    }
   ],
   "source": [
    "length = 30\n",
    "lines = []\n",
    "for i in range(len(text))[:-length]:\n",
    "    lines.append(text[i:length + i])\n",
    "print(len(text))\n",
    "print(len(lines))\n",
    "print(lines[:5])\n",
    "encoded = torch.tensor([[char2i[c] for c in l] for l in tqdm(lines)]).to(device).long()\n",
    "print(encoded.shape)\n",
    "source = encoded[:, :length // 2]\n",
    "print(source.shape)\n",
    "target = torch.cat((torch.ones(encoded.shape[0], 1).to(device).long(), encoded[:, length // 2:]), axis = 1)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Transformer\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Max sequence length: 32\n",
      "Embedding dimension: 16\n",
      "Feedforward dimension: 64\n",
      "Encoder layers: 2\n",
      "Decoder layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 20,591\n",
      "\n",
      "Epoch | Train                 | Minutes\n",
      "      | Loss     | Error Rate |\n",
      "---------------------------------------\n",
      "    1 |   1.6274 |     44.788 |      1.5\n",
      "    2 |   1.3131 |     37.988 |      3.0\n",
      "    3 |   1.2407 |     36.074 |      4.7\n",
      "    4 |   1.2031 |     35.121 |      6.2\n",
      "    5 |   1.1806 |     34.564 |      7.6\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(seq2seq)  \n",
    "\n",
    "net = seq2seq.Transformer(char2i, i2char)\n",
    "net.to(device)\n",
    "performance = net.fit(source, target, save_path = \"checkpoints/seq2seq_transformer.pt\", progress_bar = 0)\n",
    "net.save_architecture(\"architectures/seq2seq_transformer.architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_error_rate</th>\n",
       "      <th>minutes</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>model</th>\n",
       "      <th>max_sequence_length</th>\n",
       "      <th>embedding_dimension</th>\n",
       "      <th>feedforward_dimension</th>\n",
       "      <th>encoder_layers</th>\n",
       "      <th>decoder_layers</th>\n",
       "      <th>attention_heads</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.627416</td>\n",
       "      <td>44.787806</td>\n",
       "      <td>1.514237</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.313123</td>\n",
       "      <td>37.988421</td>\n",
       "      <td>3.043050</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.240702</td>\n",
       "      <td>36.074342</td>\n",
       "      <td>4.679946</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.203115</td>\n",
       "      <td>35.121373</td>\n",
       "      <td>6.160605</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.180640</td>\n",
       "      <td>34.563638</td>\n",
       "      <td>7.625306</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Transformer</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  train_error_rate   minutes  learning_rate  weight_decay  \\\n",
       "0      1    1.627416         44.787806  1.514237          0.001             0   \n",
       "1      2    1.313123         37.988421  3.043050          0.001             0   \n",
       "2      3    1.240702         36.074342  4.679946          0.001             0   \n",
       "3      4    1.203115         35.121373  6.160605          0.001             0   \n",
       "4      5    1.180640         34.563638  7.625306          0.001             0   \n",
       "\n",
       "         model  max_sequence_length  embedding_dimension  \\\n",
       "0  Transformer                   32                   16   \n",
       "1  Transformer                   32                   16   \n",
       "2  Transformer                   32                   16   \n",
       "3  Transformer                   32                   16   \n",
       "4  Transformer                   32                   16   \n",
       "\n",
       "   feedforward_dimension  encoder_layers  decoder_layers  attention_heads  \\\n",
       "0                     64               2               2                2   \n",
       "1                     64               2               2                2   \n",
       "2                     64               2               2                2   \n",
       "3                     64               2               2                2   \n",
       "4                     64               2               2                2   \n",
       "\n",
       "  activation  dropout  parameters  \n",
       "0       relu      0.0       20591  \n",
       "1       relu      0.0       20591  \n",
       "2       relu      0.0       20591  \n",
       "3       relu      0.0       20591  \n",
       "4       relu      0.0       20591  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# includes all the information about the epoch and the model, useful for reproducibility\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gu',\n",
       " 'The Project Gut',\n",
       " 'he Project Gute',\n",
       " 'e Project Guten',\n",
       " ' Project Gutenb']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input for testing\n",
    "\n",
    "net.tensor2text(source[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Transformer\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Max sequence length: 32\n",
      "Embedding dimension: 16\n",
      "Feedforward dimension: 64\n",
      "Encoder layers: 2\n",
      "Decoder layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 20,591\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e2caade8864689bf776b11844e5a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<START>nd the Black's \",\n",
       " '<START> the Black the ',\n",
       " '<START>r the Black to ',\n",
       " \"<START> the King's to \",\n",
       " \"<START>ing the King's \"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(seq2seq)    \n",
    "net = seq2seq.load_architecture(\"architectures/seq2seq_transformer.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/seq2seq_transformer.pt\"))\n",
    "net.to(device)\n",
    "\n",
    "idx, log_probabilities = net.predict(source[:5], progress_bar = 0)\n",
    "\n",
    "net.tensor2text(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Transformer\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Max sequence length: 32\n",
      "Embedding dimension: 16\n",
      "Feedforward dimension: 64\n",
      "Encoder layers: 2\n",
      "Decoder layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 20,591\n",
      "\n",
      "tensor([-16.3043, -16.0721, -17.6378, -16.8939, -14.0852], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<START>res the pawn thiis t',\n",
       " '<START>er the pawn the   su',\n",
       " '<START>r the pawn the t m\\ns',\n",
       " '<START> the pawn the p\\n teA',\n",
       " '<START>ing the pawn thiis t']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(seq2seq)    \n",
    "net = seq2seq.load_architecture(\"architectures/seq2seq_transformer.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/seq2seq_transformer.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.greedy_search(source[:5], progress_bar = False)\n",
    "\n",
    "print(log_probabilities)\n",
    "net.tensor2text(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Transformer\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Max sequence length: 32\n",
      "Embedding dimension: 16\n",
      "Feedforward dimension: 64\n",
      "Encoder layers: 2\n",
      "Decoder layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 20,591\n",
      "\n",
      "tensor([-33.5113, -24.8168, -29.2963, -34.1867, -28.5067], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<START>t pawns.\\n\\nIP Bl 7 t.',\n",
       " '<START>ed the canificich t ',\n",
       " '<START>le the\\ncastle t\\n\\nB-.',\n",
       " '<START>  pasten.\\n\\nThe t\\nW\\nt',\n",
       " '<START> King, the\\nmoved t\\nt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(seq2seq)    \n",
    "net = seq2seq.load_architecture(\"architectures/seq2seq_transformer.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/seq2seq_transformer.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.sample(source[:5], progress_bar = False)\n",
    "\n",
    "print(log_probabilities)\n",
    "net.tensor2text(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Transformer\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Max sequence length: 32\n",
      "Embedding dimension: 16\n",
      "Feedforward dimension: 64\n",
      "Encoder layers: 2\n",
      "Decoder layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 20,591\n",
      "\n",
      "tensor([[-15.0610, -15.7869, -16.5086, -16.5354, -16.6885],\n",
      "        [-17.0994, -17.3677, -17.8005, -17.8175, -17.8557],\n",
      "        [-15.5115, -15.5431, -15.5876, -15.8779, -16.3111],\n",
      "        [-15.1215, -15.3521, -15.6821, -16.3405, -16.3609],\n",
      "        [-13.0319, -13.7725, -13.9181, -14.0929, -14.1328]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\"<START>nd the Black's ct t \",\n",
       "  \"<START>nd the Black's ct s \",\n",
       "  \"<START>nd the Black's t tac\",\n",
       "  \"<START>nd the King's sforii\",\n",
       "  \"<START>nd the Black's ct t\\n\"],\n",
       " [\"<START> the King's the K t \",\n",
       "  \"<START> the King's the B t \",\n",
       "  \"<START> the King's the K t\\n\",\n",
       "  \"<START> the King's the K t-\",\n",
       "  \"<START> the King's the Bl t\"],\n",
       " ['<START>r the Black the   su',\n",
       "  '<START>r the Black the  d  ',\n",
       "  '<START>r the Black to t t t',\n",
       "  '<START>r the Black the   ch',\n",
       "  '<START>r the Black the  m\\n '],\n",
       " [\"<START> the King's Blachi\\nt\",\n",
       "  \"<START> the King's to t t t\",\n",
       "  \"<START> the King's Blact\\nt\\n\",\n",
       "  \"<START> the King's Blact\\nt \",\n",
       "  \"<START> the King's Blact-W\\n\"],\n",
       " [\"<START>ing the King's ct t \",\n",
       "  '<START>ing the Black tcoaco',\n",
       "  \"<START>ing the King's ct ct\",\n",
       "  \"<START>ing the King's ct s \",\n",
       "  \"<START>ing the King's t tt \"]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(seq2seq)    \n",
    "net = seq2seq.load_architecture(\"architectures/seq2seq_transformer.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/seq2seq_transformer.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.beam_search(source[:5], progress_bar = 0)\n",
    "\n",
    "print(log_probabilities)\n",
    "[net.tensor2text(t) for t in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
