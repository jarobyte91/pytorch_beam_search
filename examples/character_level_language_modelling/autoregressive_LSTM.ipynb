{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import autoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556949\n",
      "ï»¿The Project Gutenberg EBook of Chess Strategy, by Edward Lasker\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org/license\n",
      "\n",
      "\n",
      "Title: Chess Strategy\n",
      "\n",
      "Author: Edward Lasker\n",
      "\n",
      "Translator: J. Du Mont\n",
      "\n",
      "Release Date: November 11, 2012 [EBook #5614]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK CHESS STRATEGY ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by John Mamoun <mamounjo@umdnj.edu>, Charles\n",
      "Franks, and the Online Distributed Proofreaders website.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INFORMATION ABOUT THIS E-TEXT EDITION\n",
      "\n",
      "\n",
      "\n",
      "The following is an e-text of \"Chess Strategy,\" second edition, (1915)\n",
      "by Edward Lasker, translated by J. Du Mont.\n",
      "\n",
      "This e-text contains the 167 chess and checkers board game\n",
      "diagrams appearing in the original book, all in the form of\n",
      "ASCII line drawings. The following is a key to the diagrams:\n",
      "\n",
      "For chess pieces\n"
     ]
    }
   ],
   "source": [
    "# chess\n",
    "\n",
    "with open(\"data/chess_book.txt\", encoding = \"utf-8\") as file:\n",
    "    text = file.read()\n",
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556949"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(text)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2i = {c:i for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "char2i[\"<PAD>\"] = 0\n",
    "char2i[\"<START>\"] = 1\n",
    "char2i[\"<END>\"] = 2\n",
    "i2char = {i:c for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "i2char[0] = \"<PAD>\"\n",
    "i2char[1] = \"<START>\"\n",
    "i2char[2] = \"<END>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556949\n",
      "556932\n",
      "['\\ufeffThe Project Gute', 'The Project Guten', 'he Project Gutenb', 'e Project Gutenbe', ' Project Gutenber']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045e895b086d4418805e4a9378f1d273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=556932.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([556932, 17])\n"
     ]
    }
   ],
   "source": [
    "length = 17\n",
    "lines = []\n",
    "for i in range(len(text))[:-length]:\n",
    "    lines.append(text[i:length + i])\n",
    "print(len(text))\n",
    "print(len(lines))\n",
    "print(lines[:5])\n",
    "encoded = torch.tensor([[char2i[c] for c in l] for l in tqdm(lines)]).to(device).long()\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive LSTM\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Embedding dimension: 32\n",
      "Hidden units: 128\n",
      "Layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 230,335\n",
      "\n",
      "Epoch | Train                 | Minutes\n",
      "      | Loss     | Error Rate |\n",
      "---------------------------------------\n",
      "    1 |   1.2144 |     34.259 |     0.8\n",
      "    2 |   0.9702 |     29.068 |     1.5\n",
      "    3 |   0.9262 |     28.001 |     2.2\n",
      "    4 |   0.9010 |     27.378 |     3.0\n",
      "    5 |   0.8833 |     26.933 |     3.7\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.LSTM(char2i, i2char)\n",
    "net.to(device)\n",
    "performance = net.fit(encoded, save_path = \"checkpoints/autoregressive_lstm.pt\", progress_bar = 0)\n",
    "net.save_architecture(\"architectures/autoregressive_lstm.architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_error_rate</th>\n",
       "      <th>minutes</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>model</th>\n",
       "      <th>embedding_dimension</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.214361</td>\n",
       "      <td>34.259445</td>\n",
       "      <td>0.800802</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.970247</td>\n",
       "      <td>29.068192</td>\n",
       "      <td>1.486204</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.926212</td>\n",
       "      <td>28.000849</td>\n",
       "      <td>2.167026</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.900984</td>\n",
       "      <td>27.378410</td>\n",
       "      <td>2.986085</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.883336</td>\n",
       "      <td>26.933337</td>\n",
       "      <td>3.659008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Autoregressive LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>230335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  train_error_rate   minutes  learning_rate  weight_decay  \\\n",
       "0      1    1.214361         34.259445  0.800802          0.001             0   \n",
       "1      2    0.970247         29.068192  1.486204          0.001             0   \n",
       "2      3    0.926212         28.000849  2.167026          0.001             0   \n",
       "3      4    0.900984         27.378410  2.986085          0.001             0   \n",
       "4      5    0.883336         26.933337  3.659008          0.001             0   \n",
       "\n",
       "                 model  embedding_dimension  hidden_units  layers  dropout  \\\n",
       "0  Autoregressive LSTM                   32           128       2      0.0   \n",
       "1  Autoregressive LSTM                   32           128       2      0.0   \n",
       "2  Autoregressive LSTM                   32           128       2      0.0   \n",
       "3  Autoregressive LSTM                   32           128       2      0.0   \n",
       "4  Autoregressive LSTM                   32           128       2      0.0   \n",
       "\n",
       "   parameters  \n",
       "0      230335  \n",
       "1      230335  \n",
       "2      230335  \n",
       "3      230335  \n",
       "4      230335  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# includes all the information about the epoch and the model, useful for reproducibility\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gute',\n",
       " 'The Project Guten',\n",
       " 'he Project Gutenb',\n",
       " 'e Project Gutenbe',\n",
       " ' Project Gutenber']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the input for testing\n",
    "\n",
    "net.tensor2text(encoded[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive LSTM\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Embedding dimension: 32\n",
      "Hidden units: 128\n",
      "Layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 230,335\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gutenberg-tm electronic ',\n",
       " 'The Project Gutenberg-tm electronic w',\n",
       " 'he Project Gutenberg-tm electronic wo',\n",
       " 'e Project Gutenberg-tm electronic wor',\n",
       " ' Project Gutenberg-tm electronic work']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.load_architecture(\"architectures/autoregressive_lstm.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/autoregressive_lstm.pt\"))\n",
    "net.to(device)\n",
    "idx, log_probabilities = net.predict(encoded[:5], main_progress_bar = False, progress_bar = 0)\n",
    "\n",
    "net.tensor2text(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive LSTM\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Embedding dimension: 32\n",
      "Hidden units: 128\n",
      "Layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 230,335\n",
      "\n",
      "tensor([-2.4522, -2.5134, -2.5617, -2.5795, -2.5688], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gutenberg-tm electronic ',\n",
       " 'The Project Gutenberg-tm electronic w',\n",
       " 'he Project Gutenberg-tm electronic wo',\n",
       " 'e Project Gutenberg-tm electronic wor',\n",
       " ' Project Gutenberg-tm electronic work']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.load_architecture(\"architectures/autoregressive_lstm.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/autoregressive_lstm.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.greedy_search(encoded[:5], progress_bar = False)\n",
    "\n",
    "print(log_probabilities)\n",
    "net.tensor2text(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive LSTM\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Embedding dimension: 32\n",
      "Hidden units: 128\n",
      "Layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 230,335\n",
      "\n",
      "tensor([-10.0899, -24.9052, -11.4014, -22.5374,  -9.9133], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gutenberg-tm to exchange',\n",
       " 'The Project Gutenberg-tm file wit as ',\n",
       " 'he Project Gutenberg Literary\\n[Footno',\n",
       " 'e Project Gutenberg\" as to avoid a gr',\n",
       " ' Project Gutenberg-tm License\\n\\n      ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.load_architecture(\"architectures/autoregressive_lstm.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/autoregressive_lstm.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.sample(encoded[:5], progress_bar = False)\n",
    "\n",
    "print(log_probabilities)\n",
    "net.tensor2text(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Autoregressive LSTM\n",
      "Tokens in the in vocabulary: 95\n",
      "Tokens in the out vocabulary: 95\n",
      "Embedding dimension: 32\n",
      "Hidden units: 128\n",
      "Layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 230,335\n",
      "\n",
      "tensor([[-2.4522, -3.0231, -4.5768, -4.8534, -5.7860],\n",
      "        [-2.5134, -3.1163, -4.6800, -5.7090, -5.9480],\n",
      "        [-2.5617, -3.1450, -4.7755, -5.6018, -5.9589],\n",
      "        [-2.5795, -3.2951, -4.8843, -5.8266, -5.9226],\n",
      "        [-2.5688, -3.4184, -4.8764, -6.4988, -6.7574]], device='cuda:0')\n",
      "[['\\ufeffThe Project Gutenberg-tm electronic ',\n",
      "  '\\ufeffThe Project Gutenberg Literary Archi',\n",
      "  '\\ufeffThe Project Gutenberg-tm electronic\\n',\n",
      "  '\\ufeffThe Project Gutenberg-tm trademark, ',\n",
      "  '\\ufeffThe Project Gutenberg-tm trademark. '],\n",
      " ['The Project Gutenberg-tm electronic w',\n",
      "  'The Project Gutenberg Literary Archiv',\n",
      "  'The Project Gutenberg-tm electronic\\nw',\n",
      "  'The Project Gutenberg-tm trademark, a',\n",
      "  'The Project Gutenberg Literary Archil'],\n",
      " ['he Project Gutenberg-tm electronic wo',\n",
      "  'he Project Gutenberg Literary Archive',\n",
      "  'he Project Gutenberg-tm electronic\\nwo',\n",
      "  'he Project Gutenberg-tm electronic wa',\n",
      "  'he Project Gutenberg-tm trademark, an'],\n",
      " ['e Project Gutenberg-tm electronic wor',\n",
      "  'e Project Gutenberg Literary Archive ',\n",
      "  'e Project Gutenberg-tm electronic\\nwor',\n",
      "  'e Project Gutenberg Literary Archive\\n',\n",
      "  'e Project Gutenberg-tm electronic war'],\n",
      " [' Project Gutenberg-tm electronic work',\n",
      "  ' Project Gutenberg Literary Archive F',\n",
      "  ' Project Gutenberg-tm electronic\\nwork',\n",
      "  ' Project Gutenberg Literary Archive\\nF',\n",
      "  ' Project Gutenberg Literary Archive E']]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(autoregressive)    \n",
    "net = autoregressive.load_architecture(\"architectures/autoregressive_lstm.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/autoregressive_lstm.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.beam_search(encoded[:5], progress_bar = False)\n",
    "\n",
    "print(log_probabilities)\n",
    "pprint([net.tensor2text(t) for t in indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
