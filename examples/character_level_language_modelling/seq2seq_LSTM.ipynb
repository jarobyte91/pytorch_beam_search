{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556949\n",
      "ï»¿The Project Gutenberg EBook of Chess Strategy, by Edward Lasker\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org/license\n",
      "\n",
      "\n",
      "Title: Chess Strategy\n",
      "\n",
      "Author: Edward Lasker\n",
      "\n",
      "Translator: J. Du Mont\n",
      "\n",
      "Release Date: November 11, 2012 [EBook #5614]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK CHESS STRATEGY ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by John Mamoun <mamounjo@umdnj.edu>, Charles\n",
      "Franks, and the Online Distributed Proofreaders website.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INFORMATION ABOUT THIS E-TEXT EDITION\n",
      "\n",
      "\n",
      "\n",
      "The following is an e-text of \"Chess Strategy,\" second edition, (1915)\n",
      "by Edward Lasker, translated by J. Du Mont.\n",
      "\n",
      "This e-text contains the 167 chess and checkers board game\n",
      "diagrams appearing in the original book, all in the form of\n",
      "ASCII line drawings. The following is a key to the diagrams:\n",
      "\n",
      "For chess pieces\n"
     ]
    }
   ],
   "source": [
    "# chess\n",
    "\n",
    "with open(\"data/chess_book.txt\", encoding = \"utf-8\") as file:\n",
    "    text = file.read()\n",
    "print(len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556949"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(text)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "char2i = {c:i for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "char2i[\"<PAD>\"] = 0\n",
    "char2i[\"<START>\"] = 1\n",
    "char2i[\"<END>\"] = 2\n",
    "print(len(char2i))\n",
    "i2char = {i:c for i, c in enumerate(sorted(vocabulary), 3)}\n",
    "i2char[0] = \"<PAD>\"\n",
    "i2char[1] = \"<START>\"\n",
    "i2char[2] = \"<END>\"\n",
    "print(len(i2char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556949\n",
      "556917\n",
      "['\\ufeffThe Project Gutenberg EBook of ', 'The Project Gutenberg EBook of C', 'he Project Gutenberg EBook of Ch', 'e Project Gutenberg EBook of Che', ' Project Gutenberg EBook of Ches']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec26c95ece84950953deba4bed959fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=556917.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([556917, 32])\n",
      "torch.Size([556917, 16])\n",
      "torch.Size([556917, 17])\n"
     ]
    }
   ],
   "source": [
    "length = 32\n",
    "lines = []\n",
    "for i in range(len(text))[:-length]:\n",
    "    lines.append(text[i:length + i])\n",
    "print(len(text))\n",
    "print(len(lines))\n",
    "print(lines[:5])\n",
    "encoded = torch.tensor([[char2i[c] for c in l] for l in tqdm(lines)]).to(device).long()\n",
    "print(encoded.shape)\n",
    "source_1 = encoded[:, :length // 2]\n",
    "print(source_1.shape)\n",
    "target_1 = torch.cat((torch.ones(encoded.shape[0], 1).to(device).long(), encoded[:, length // 2:]), axis = 1)\n",
    "print(target_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq LSTM\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Encoder embedding dimension: 32\n",
      "Decoder embedding dimension: 32\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 2\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 579,487\n",
      "\n",
      "Epoch | Train                 | Minutes\n",
      "      | Loss     | Error Rate |\n",
      "---------------------------------------\n",
      "    1 |   1.2396 |     35.408 |      1.4\n",
      "    2 |   0.8837 |     27.066 |      2.8\n",
      "    3 |   0.8020 |     24.921 |      4.2\n",
      "    4 |   0.7540 |     23.642 |      5.7\n",
      "    5 |   0.7205 |     22.732 |      7.1\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(seq2seq)  \n",
    "\n",
    "net = seq2seq.LSTM(char2i, i2char)\n",
    "net.to(device)\n",
    "performance = net.fit(source_1, target_1, save_path = \"checkpoints/seq2seq_lstm.pt\", progress_bar = 0)\n",
    "net.save_architecture(\"architectures/seq2seq_lstm.architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_error_rate</th>\n",
       "      <th>minutes</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>model</th>\n",
       "      <th>encoder_embedding_dimension</th>\n",
       "      <th>decoder_embedding_dimension</th>\n",
       "      <th>encoder_hidden_units</th>\n",
       "      <th>encoder_layers</th>\n",
       "      <th>decoder_hidden_units</th>\n",
       "      <th>decoder_layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.239611</td>\n",
       "      <td>35.408093</td>\n",
       "      <td>1.413818</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Seq2Seq LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>579487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.883674</td>\n",
       "      <td>27.066017</td>\n",
       "      <td>2.830199</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Seq2Seq LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>579487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.802045</td>\n",
       "      <td>24.920915</td>\n",
       "      <td>4.249755</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Seq2Seq LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>579487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.753978</td>\n",
       "      <td>23.641730</td>\n",
       "      <td>5.671105</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Seq2Seq LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>579487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.720463</td>\n",
       "      <td>22.731978</td>\n",
       "      <td>7.087438</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0</td>\n",
       "      <td>Seq2Seq LSTM</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>579487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  train_error_rate   minutes  learning_rate  weight_decay  \\\n",
       "0      1    1.239611         35.408093  1.413818          0.001             0   \n",
       "1      2    0.883674         27.066017  2.830199          0.001             0   \n",
       "2      3    0.802045         24.920915  4.249755          0.001             0   \n",
       "3      4    0.753978         23.641730  5.671105          0.001             0   \n",
       "4      5    0.720463         22.731978  7.087438          0.001             0   \n",
       "\n",
       "          model  encoder_embedding_dimension  decoder_embedding_dimension  \\\n",
       "0  Seq2Seq LSTM                           32                           32   \n",
       "1  Seq2Seq LSTM                           32                           32   \n",
       "2  Seq2Seq LSTM                           32                           32   \n",
       "3  Seq2Seq LSTM                           32                           32   \n",
       "4  Seq2Seq LSTM                           32                           32   \n",
       "\n",
       "   encoder_hidden_units  encoder_layers  decoder_hidden_units  decoder_layers  \\\n",
       "0                   128               2                   128               2   \n",
       "1                   128               2                   128               2   \n",
       "2                   128               2                   128               2   \n",
       "3                   128               2                   128               2   \n",
       "4                   128               2                   128               2   \n",
       "\n",
       "   dropout  parameters  \n",
       "0      0.0      579487  \n",
       "1      0.0      579487  \n",
       "2      0.0      579487  \n",
       "3      0.0      579487  \n",
       "4      0.0      579487  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# includes all the information about the epoch and the model, useful for reproducibility\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe Project Gut',\n",
       " 'The Project Gute',\n",
       " 'he Project Guten',\n",
       " 'e Project Gutenb',\n",
       " ' Project Gutenbe']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the input for testing\n",
    "\n",
    "net.tensor2text(source_1[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq LSTM\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Encoder embedding dimension: 32\n",
      "Decoder embedding dimension: 32\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 2\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 579,487\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<START>enberg-tm electr',\n",
       " '<START>nberg-tm electro',\n",
       " '<START>berg-tm electron',\n",
       " '<START>erg-tm electroni',\n",
       " '<START>rg-tm electronic']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(seq2seq)    \n",
    "net = seq2seq.load_architecture(\"architectures/seq2seq_lstm.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/seq2seq_lstm.pt\"))\n",
    "net.to(device)\n",
    "\n",
    "idx, log_probabilities = net.predict(source_1[:5], main_progress_bar = False, progress_bar = 0)\n",
    "\n",
    "net.tensor2text(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# greedy_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq LSTM\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Encoder embedding dimension: 32\n",
      "Decoder embedding dimension: 32\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 2\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 579,487\n",
      "\n",
      "tensor([-2.3400, -2.2834, -2.2556, -2.9405, -2.2111], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<START>enberg-tm electronic',\n",
       " '<START>nberg-tm electronic ',\n",
       " '<START>berg-tm electronic w',\n",
       " '<START>erg-tm electronic wo',\n",
       " '<START>rg-tm electronic wor']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(seq2seq)    \n",
    "net = seq2seq.load_architecture(\"architectures/seq2seq_lstm.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/seq2seq_lstm.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.greedy_search(source_1[:5], progress_bar = False)\n",
    "\n",
    "print(log_probabilities)\n",
    "net.tensor2text(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq LSTM\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Encoder embedding dimension: 32\n",
      "Decoder embedding dimension: 32\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 2\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 579,487\n",
      "\n",
      "tensor([-19.2702, -21.5866, -31.3541, -11.2698,  -9.0979], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<START>enberg-tm threlegree',\n",
       " '<START>ptony his distrodect',\n",
       " '<START>berg (108 is enoorat',\n",
       " '<START>erg Literary Johused',\n",
       " '<START>rg-tm\\nelectronic the']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(seq2seq)    \n",
    "net = seq2seq.load_architecture(\"architectures/seq2seq_lstm.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/seq2seq_lstm.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.sample(source_1[:5], progress_bar = False)\n",
    "\n",
    "print(log_probabilities)\n",
    "net.tensor2text(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beam_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq LSTM\n",
      "Tokens in the input vocabulary: 95\n",
      "Tokens in the output vocabulary: 95\n",
      "Encoder embedding dimension: 32\n",
      "Decoder embedding dimension: 32\n",
      "Encoder hidden units: 128\n",
      "Encoder layers: 2\n",
      "Decoder hidden units: 128\n",
      "Decoder layers: 2\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 579,487\n",
      "\n",
      "tensor([[-2.3400, -2.6164, -3.6843, -5.1370, -5.5572],\n",
      "        [-2.2834, -3.2239, -3.3584, -3.8615, -4.9356],\n",
      "        [-2.2556, -3.1783, -3.9974, -4.1785, -4.9287],\n",
      "        [-2.9405, -3.6931, -4.0829, -4.4183, -5.1069],\n",
      "        [-2.2111, -3.1253, -3.6750, -4.4433, -4.7890]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['<START>enberg-tm electronic',\n",
       "  '<START>enberg Literary Arch',\n",
       "  '<START>enberg-tm\\nelectronic',\n",
       "  '<START>enberg-tm electrowin',\n",
       "  '<START>enberg-tm electrogra'],\n",
       " ['<START>nberg-tm electronic ',\n",
       "  '<START>nberg Literary Archi',\n",
       "  '<START>nberg-tm\\nelectronic ',\n",
       "  '<START>nberg-tm electronic\\n',\n",
       "  '<START>nberg-tm\\nelectronic\\n'],\n",
       " ['<START>berg-tm electronic w',\n",
       "  '<START>berg-tm\\nelectronic w',\n",
       "  '<START>berg-tm electronic\\nw',\n",
       "  '<START>berg Literary Archiv',\n",
       "  '<START>berg-tm\\nelectronic\\nw'],\n",
       " ['<START>erg-tm electronic wo',\n",
       "  '<START>erg Literary Archive',\n",
       "  '<START>erg-tm\\nelectronic wo',\n",
       "  '<START>erg-tm electronic\\nwo',\n",
       "  '<START>erg-tm electronic an'],\n",
       " ['<START>rg-tm electronic wor',\n",
       "  '<START>rg-tm\\nelectronic wor',\n",
       "  '<START>rg-tm electronic\\nwor',\n",
       "  '<START>rg Literary Archive ',\n",
       "  '<START>rg-tm electronic and']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(seq2seq)    \n",
    "net = seq2seq.load_architecture(\"architectures/seq2seq_lstm.architecture\")\n",
    "net.load_state_dict(torch.load(\"checkpoints/seq2seq_lstm.pt\"))\n",
    "net.to(device)\n",
    "indexes, log_probabilities = net.beam_search(source_1[:5], progress_bar = 0)\n",
    "\n",
    "print(log_probabilities)\n",
    "[net.tensor2text(t) for t in indexes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
